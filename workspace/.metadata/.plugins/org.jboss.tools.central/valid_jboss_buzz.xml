<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">MicroProfile LRA: A Comprehensive Guide</title><link rel="alternate" href="https://www.mastertheboss.com/eclipse/eclipse-microservices/microprofile-lra-a-comprehensive-guide/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/eclipse/eclipse-microservices/microprofile-lra-a-comprehensive-guide/</id><updated>2023-05-24T21:01:14Z</updated><content type="html">In today’s distributed and microservices-based architectures, ensuring data consistency and coordination across multiple services can be challenging. This is where MicroProfile LRA (Long Running Actions) comes into play. In this tutorial, we will explore the fundamentals of MicroProfile LRA, how to use it effectively, and compare it with other standards such as distributed transactions. What ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Using MicroProfile LRA in WildFly</title><link rel="alternate" href="https://wildfly.org//news/2023/05/24/MicroProfile-LRA/" /><author><name>Martin Stefanko</name></author><id>https://wildfly.org//news/2023/05/24/MicroProfile-LRA/</id><updated>2023-05-24T00:00:00Z</updated><content type="html">is a specification that defines the protocol and an API for the distributed transactions based on the saga pattern and user-defined compensations. In WildFly 28.0.0.Final, we introduced the integration of which implements this specification. In this guide, we look into how you can enable LRA in your WildFly distribution and how you can use LRA in your applications. OVERVIEW OF THE LRA PROTOCOL We provide only a high-level overview of the LRA protocol in this post. The full overview of the protocol is available at . In LRA, the specification API utilizes annotations from the org.eclipse.microprofile.lra.annotation package. The main annotation is the @LRA which controls the life cycle of the LRA. It’s use might seem similar to the use of @Transactional annotation from JTA, however, the transaction characteristics differ greatly. If you are interested in the comparison of the saga pattern to the ACID transactions, you can find an explanation in this talk from DevoxxUK - . The Narayana implementation utilizes the coordinator orchestration of the LRAs. The LRA coordinator is a standalone service that is responsible for the management operations of the LRAs started in the system. When any LRA participant (user service) wants to start a new LRA, it contacts the LRA coordinator that in turn returns the LRA ID of the newly started LRA that can be propagated by the LRA participant to any other services. When an LRA-aware service receives the LRA ID, it can optionally enlist within the same LRA which is again done by the enlistment call to the coordinator. When the LRA finishes (success or failure), the LRA coordinator is responsible for invocations of the completions or the compensations callbacks of all enlisted LRA participants. ENABLING MICROPROFILE LRA SUBSYSTEMS The integration of the LRA specification is included in two separate subsystems: * microprofile-lra-coordinator - The LRA coordinator responsible for starting, managing, and recovery of the LRAs. * microprofile-lra-participant - The client library utilized in user deployments to participate in the distributed LRAs and define compensation and completition callbacks. REQUIRED EXTENSIONS AND SUBSYSTEMS CONFIGURATION The LRA extensions are not included in the standard configurations included with WildFly application server. They need to be explitcly enabled either in the configuration XML or by using CLI operations: [standalone@localhost:9990 /] /extension=org.wildfly.extension.microprofile.lra-coordinator:add {"outcome" =&gt; "success"} [standalone@localhost:9990 /] /subsystem=microprofile-lra-coordinator:add { "outcome" =&gt; "success", "response-headers" =&gt; { "operation-requires-reload" =&gt; true, "process-state" =&gt; "reload-required" } } [standalone@localhost:9990 /] /extension=org.wildfly.extension.microprofile.lra-participant:add {"outcome" =&gt; "success"} [standalone@localhost:9990 /] /subsystem=microprofile-lra-participant:add { "outcome" =&gt; "success", "response-headers" =&gt; { "operation-requires-reload" =&gt; true, "process-state" =&gt; "reload-required" } } [standalone@localhost:9990 /] reload RUNNING LRA COORDINATOR IN A DOCKER CONTAINER The LRA coordinator is also provided as a standalone Docker image that you can simply run with the following command: $ docker run -p 8080:8080 quay.io/jbosstm/lra-coordinator USING LRA IN USER DEPLOYMENTS The @LRA annotation can be placed on any JAX-RS method to declare that the LRA should be started before the method is entered and closed (finished successfully) when the method ends. By default, if the JAX-RS method returns any of the 4xx or 5xx error HTTP status codes the LRA will be cancelled instead. @LRA @GET @Path("/doInLRA") public Response doInLRA(@HeaderParam(LRA.LRA_HTTP_CONTEXT_HEADER) String lraId) { LOG.info("Work LRA ID = " + lraId); ... When LRA closes successfully, the LRA coordinator calls the completion callback if the participant defined it: @Complete @PUT @Path("/complete") public Response complete(@HeaderParam(LRA.LRA_HTTP_CONTEXT_HEADER) String lraId) { LOG.info("Complete ID = " + lraId); ... Or, in the case of LRA cancel, the compensation callback will be invoked instead: @Compensate @PUT @Path("/compensate") public Response compensate(@HeaderParam(LRA.LRA_HTTP_CONTEXT_HEADER) String lraId) { LOG.info("Compensate ID = " + lraId); ... The full example is available at . If you deploy this application to WildFly (28.0.0+) with both microprofile-lra-coordinator and microprofile-lra-participant subsystems enabled, you can make the following HTTP invocation to see how the coordinator invokes the complete callbacks or the compensation callbacks of the two defined participants: $ curl localhost:8080/lra-participant/lra-participant-1/doInLRA # in WFLY console log 15:14:50,128 INFO [io.xstefank.LRAParticipant1] (default task-1) Work LRA ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_14 15:14:50,158 INFO [io.xstefank.LRAParticipant2] (default task-2) Work LRA ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_14 15:14:50,183 INFO [io.xstefank.LRAParticipant1] (default task-3) Complete ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_14 15:14:50,191 INFO [io.xstefank.LRAParticipant2] (default task-3) Complete ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_14 $ curl "localhost:8080/lra-participant/lra-participant-1/doInLRA?fail=true" # in WFLY console log 15:15:33,516 INFO [io.xstefank.LRAParticipant1] (default task-1) Work LRA ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_1c 15:15:33,531 INFO [io.xstefank.LRAParticipant2] (default task-2) Work LRA ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_1c 15:15:33,543 INFO [io.xstefank.LRAParticipant1] (default task-3) Compensate ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_1c 15:15:33,550 INFO [io.xstefank.LRAParticipant2] (default task-3) Compensate ID = http://localhost:8080/lra-coordinator/lra-coordinator/0_ffff0aca8851_-3330598e_646cbc18_1c You can also always check the currently active LRAs with a direct call to the coordinator API: $ curl localhost:8080/lra-coordinator/lra-coordinator []% CONCLUSION In this post, we showed you how to configure and use the MicroProfile LRA specification in your WildFly applications. LRA provides a very broad feature set which we can’t cover here. If you are interested in learning more, you can find the full specification at .</content><dc:creator>Martin Stefanko</dc:creator></entry><entry><title>Podman Desktop 1.0: Local container development made easy</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available" /><author><name>Stevan Le Meur</name></author><id>40318020-209c-48e6-8f98-55b451b02daf</id><updated>2023-05-23T13:45:50Z</updated><published>2023-05-23T13:45:50Z</published><summary type="html">&lt;p&gt;As containerization continues to gain popularity in the world of enterprise software development, there is also growing demand for tools and technologies that make &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; management more accessible and efficient. One such tool is Podman Desktop, which provides a user-friendly interface for managing containers and working with &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; from a local machine (Figure 1).&lt;/p&gt; &lt;p&gt;After months of hard work, we are excited to announce the general availability (GA) of Podman Desktop 1.0. Let's explore what Podman Desktop is and why it can be advantageous for enterprise developers.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-ga.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-ga.png?itok=04C__0YE" width="600" height="351" alt="Screenshots of various Podman Desktop interface elements" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Podman Desktop interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Why use Podman Desktop?&lt;/h2&gt; &lt;p&gt;Podman Desktop is a container management tool that lets developers easily create, manage, and deploy containers on their local machine. Podman Desktop downloads, installs, and abstracts away the configuration of the underlying environment. This makes it a lightweight and efficient option for container management without the overhead of having to administer everything locally.&lt;/p&gt; &lt;h3&gt;Easily manage multiple containers&lt;/h3&gt; &lt;p&gt;The main advantage of using a UI like Podman Desktop for container management, especially for enterprise developers, is that it simplifies the process of working with containers. You can easily view and manage all containers in one place rather than having to remember and type out complex command-line commands. This saves time and reduces the risk of errors when managing multiple containers or complex container configurations.&lt;/p&gt; &lt;h3&gt;Onboard developers faster&lt;/h3&gt; &lt;p&gt;Another advantage of using Podman Desktop is that it can help developers who are new to containerization get started more easily. The user-friendly interface and simplified management process make it easier for developers who might be intimidated by the command-line interface of other container management tools to get started with containerization. This can help organizations onboard new developers more quickly and reduce the learning curve for containerization.&lt;/p&gt; &lt;h3&gt;Work natively with Kubernetes&lt;/h3&gt; &lt;p&gt;For developers interested in Kubernetes or targeting it as a deployment platform, Podman Desktop provides the ability to natively work with Kubernetes objects, which helps to gradually and naturally transition from containers to Kubernetes. Podman Desktop also provides an out-of-the-box Kubernetes environment based on Kind. This means that developers can create and test applications in an environment that closely mirrors production, preventing configuration changes between development and production and ensuring a smooth transition from one environment to another.&lt;/p&gt; &lt;h2&gt;Key features&lt;/h2&gt; &lt;p&gt;Podman Desktop includes many features that streamline container workflows, ensuring a smooth and efficient developer experience. &lt;/p&gt; &lt;h3&gt;Podman and Kubernetes Local&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Install and run anywhere: Windows, macOS, and &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Configure and install via Podman, Kind, &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt;, &lt;a href="https://developers.redhat.com/developer-sandbox" target="_blank"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Keep Podman and other dependencies up to date&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Containers and pods&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Build, run, manage, and debug containers and pods&lt;/li&gt; &lt;li aria-level="2"&gt;Run pods with or without Kubernetes&lt;/li&gt; &lt;li aria-level="2"&gt;Use the built-in terminal to ssh into containers&lt;/li&gt; &lt;li aria-level="2"&gt;Manage multiple container engines&lt;/li&gt; &lt;li aria-level="2"&gt;Manage volumes&lt;/li&gt; &lt;li aria-level="2"&gt;Compatabile with Docker Compose&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Kubernetes&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Play Kubernetes YAML&lt;/li&gt; &lt;li aria-level="2"&gt;Generate Kubernetes YAML from Pods&lt;/li&gt; &lt;li aria-level="2"&gt;Podify and Kubify: Convert containers to pods and Kubernetes&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Enterprise readiness&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;VPN and proxies configuration&lt;/li&gt; &lt;li aria-level="2"&gt;Image registry management &lt;ul&gt;&lt;li aria-level="3"&gt;Configure multiple OCI registries&lt;/li&gt; &lt;li aria-level="3"&gt;Pull, tag, and push images&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;AirGapped installation&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Bridge between local and remote environments&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Connect and deploy to remote OpenShift clusters&lt;/li&gt; &lt;li aria-level="2"&gt;Enable remote managed services locally&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Extensibility&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Possibility to extend the container engines or Kubernetes providers&lt;/li&gt; &lt;li aria-level="2"&gt;Extension points to add actions, menus, configurations and enrich the UI with specific capabilities&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;From OpenShift Local to production&lt;/h2&gt; &lt;p&gt;One of the unique features of Podman Desktop is its integration with &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt; (Figure 2), which lets users develop and test applications locally using the same container images and environment as they would in a production  environment on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. This means you can ensure that your applications will run smoothly in an OpenShift environment before deploying them.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-openshift-local.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-openshift-local.png?itok=y-UGHnMb" width="600" height="436" alt="podman-desktop-openshift-local-integration" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Initializing OpenShift Local in Podman Desktop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;OpenShift Local is essentially a single-node OpenShift cluster that runs locally on the developer's machine. It is based on the same technology as OpenShift and provides a consistent environment for developing and testing applications.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2022/05/10/whats-new-openshift-local-20"&gt;What’s new in OpenShift Local 2.0&lt;/a&gt; ]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Podman Desktop 1.0 includes an extension for OpenShift Local, which you can install from the dashboard (Figure 3) or Settings (Figure 4).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift-local-podman-desktop-extension.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift-local-podman-desktop-extension.png?itok=GKTvGcPl" width="600" height="368" alt="Installing the OpenShift Local extension from the dashboard in Podman Desktop." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Installing the OpenShift Local extension from the dashboard in Podman Desktop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-openshift-local-settings.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-openshift-local-settings.png?itok=srcXSYsl" width="600" height="436" alt="Installing the OpenShift Local extension from the Settings page in Podman Desktop." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Installing the OpenShift Local extension from the Settings page in Podman Desktop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once installed, the extension provides you the ability to configure the different presets you can use for OpenShift Local:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Single-node OpenShift&lt;/strong&gt; powered by OpenShift Container Platform &lt;ul&gt;&lt;li aria-level="2"&gt;Full services set&lt;/li&gt; &lt;li aria-level="2"&gt;Complete and more resource-intensive&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Light and optimized&lt;/strong&gt; (experimental) powered by Microshift &lt;ul&gt;&lt;li aria-level="2"&gt;Minimal services set&lt;/li&gt; &lt;li aria-level="2"&gt;Fast and lightweight&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;With Podman Desktop, you can ease the transition of an application from a local environment to a remote production OpenShift environment.&lt;/p&gt; &lt;h2&gt;Local Kubernetes with Kind&lt;/h2&gt; &lt;p&gt;If you are looking for an even lighter local runtime, Podman Desktop offers the option to use &lt;a href="https://podman-desktop.io/docs/kubernetes/kind"&gt;Kind&lt;/a&gt; as an alternative container orchestration tool. The use of Kind provides several benefits for developers who are striving to create a development environment that closely mirrors production.&lt;/p&gt; &lt;p&gt;One key advantage of using Kind is that it allows developers to create a multi-node Kubernetes cluster on their local machine. Unlike Docker Compose, which is designed for single-node environments, Kind provides a more realistic environment for testing applications that will be deployed on multiple nodes in production. With Kind, developers can simulate a more complex environment and ensure their applications are ready for production deployment.&lt;/p&gt; &lt;p&gt;Kind creates Kubernetes clusters as containers, which means that developers can easily test their applications with the latest version of Kubernetes without having to install and configure it manually. Kind also simplifies the process of spinning up and tearing down Kubernetes clusters, which can save developers time and reduce the risk of configuration errors.&lt;/p&gt; &lt;h3&gt;Creating Kind clusters in Podman Desktop&lt;/h3&gt; &lt;p&gt;To create a Kind cluster from Podman Desktop, go into the &lt;strong&gt;Settings → Resources&lt;/strong&gt; page; you'll find a section enabling you to configure a cluster (see Figure 5).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ingress-controller-kind-podman.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/ingress-controller-kind-podman.png?itok=Dt1-T6w-" width="600" height="419" alt="Configuring a Kind cluster from the Podman Desktop Resources page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Configuring a Kind cluster from the Podman Desktop Resources page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can choose a name for the cluster and specify the ports to be used by the cluster. You can also set up an Ingress controller (with the project Contour), as shown in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-kind-cluster.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-kind-cluster.png?itok=D-yhT9Cc" width="600" height="419" alt="Configuring a Kind cluster in Podman Desktop" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Configuring the cluster and enabling an Ingress controller in Podman Desktop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the cluster is up and running, it will be your default kubecontext. You can interact with the cluster either from Podman Desktop or the other tools you are using, such as kubectl. If you need to interact with the cluster, you can open the terminal directly from within Podman Desktop.&lt;/p&gt; &lt;p&gt;Using Kind with Podman Desktop allows developers to ensure their local development environment closely mirrors their production environment. By using the same container orchestration tool and environment as production, you can avoid issues that might arise when deploying applications to production, such as configuration differences or compatibility issues. Using Kind with Podman Desktop creates a more efficient and reliable development process that ultimately leads to more successful production deployments.&lt;/p&gt; &lt;h2&gt;Developer Sandbox for Red Hat OpenShift extension&lt;/h2&gt; &lt;p&gt;We also added an extension for the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which you can install from the dashboard or the Settings (Figure 7). The Developer Sandbox for Red Hat OpenShift is a free, cloud-based OpenShift environment for developers to create, build, and deploy applications on OpenShift at no cost. With Podman Desktop, you can easily connect to the Developer Sandbox from your local machine and deploy your applications to the cloud-based environment.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-developer-sandbox.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-developer-sandbox.png?itok=tTrUFgIL" width="600" height="467" alt="Accessing the Developer Sandbox for Red Hat OpenShift using the extension for Podman Desktop." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Accessing the Developer Sandbox for Red Hat OpenShift using the extension for Podman Desktop.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To use Podman Desktop with the Developer Sandbox, you must first &lt;a href="https://developers.redhat.com/articles/2023/03/30/how-access-developer-sandbox-red-hat-openshift"&gt;sign up for a free account&lt;/a&gt; and create a new project in the OpenShift web console. You can then use Podman Desktop to build and test your application locally, using the same container images and environment as you would in production. Once the application is ready for deployment, you can use Podman Desktop to push the container images to the OpenShift registry and deploy your application to the OpenShift environment.&lt;/p&gt; &lt;p&gt;Using Podman Desktop with the Developer Sandbox for Red Hat OpenShift provides several benefits for enterprise developers. It lets you test your applications in a managed Kubernetes environment without having to set up and manage your own infrastructure. It also provides a consistent environment for testing and deployment, which can help reduce the risk of configuration errors and compatibility issues.&lt;/p&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;We plan to improve Podman Desktop with more capabilities to help you work with containers and facilitate the transition from containers to Kubernetes. We are also planning to extend the support of Kubernetes objects, enabling you to create, run, debug, and more easily manage all different components of your applications.&lt;/p&gt; &lt;p&gt;For developers targeting OpenShift, you'll see more tools integrated with Podman Desktop that make it smoother and more efficient to build your application locally and run/debug it in an environment consistent with production. &lt;/p&gt; &lt;h2&gt;More information&lt;/h2&gt; &lt;p&gt;Podman Desktop continues its momentum, and we are excited about the road ahead. Visit &lt;a href="https://podman-desktop.io"&gt;podman-desktop.io&lt;/a&gt; to download the tool and learn more about the project.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Read more about Podman Desktop on Red Hat Developer: &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Visit the Podman website: &lt;a href="http://podman.io"&gt;podman.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Explore the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/containers/podman-desktop"&gt;Support us by giving a ⭐️&lt;/a&gt;!&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available" title="Podman Desktop 1.0: Local container development made easy"&gt;Podman Desktop 1.0: Local container development made easy&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Stevan Le Meur</dc:creator><dc:date>2023-05-23T13:45:50Z</dc:date></entry><entry><title>A developer’s guide to Red Hat Developer Hub and Janus</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/23/developers-guide-red-hat-developer-hub-and-janus" /><author><name>Ian Lawson, Yaina Williams</name></author><id>bfe4f902-f799-400b-a548-bc094ce78c41</id><updated>2023-05-23T13:00:00Z</updated><published>2023-05-23T13:00:00Z</published><summary type="html">&lt;p&gt;This article introduces the new &lt;a href="https://developers.redhat.com/products/developer-hub/overview"&gt;Red Hat Developer Hub&lt;/a&gt; and Janus project to address the challenges IT organizations face in the development process. A developer’s work can be fraught with disparate development systems and distributed teams, and organizations with multiple development teams often struggle with competing priorities, diverse tools and technologies, and establishing best practices.&lt;/p&gt; &lt;p&gt;These challenges make it difficult to quickly start development and adhere to multiple security and compliance standards. A unified platform that can consolidate these elements of the development process and foster internal collaboration will enable development teams to focus on rapidly enhancing code and functionality to efficiently build high-quality software.&lt;/p&gt; &lt;h2&gt;Simplifying the inner loop for developers&lt;/h2&gt; &lt;p&gt;Simplifying the inner and outer loop model is also an important part of improving the development process. The article &lt;a href="https://developers.redhat.com/articles/2022/12/16/standardizing-application-delivery-openshift#"&gt;Standardizing application delivery with OpenShift&lt;/a&gt; explains the concept and purpose of dividing the development process into two loops. In the inner loop, the developer works on the code. In the outer loop, developers push the code to version control for automation, testing, and deployment until it is ready for release. Developers need a simple inner loop so that they can focus on finding software solutions, not configuring tools.&lt;/p&gt; &lt;p&gt;Developers need a single place, like a hub, where they can find resources, utilize and generate shareable components, and follow Golden Path templates&lt;em&gt; &lt;/em&gt;(templated paths to quick and easy software development). Read on to learn about the new Red Hat Developer Hub and Janus project, and how they simplify the developer workflow.&lt;/p&gt; &lt;h2&gt;Overview of Backstage and Project Janus&lt;/h2&gt; &lt;p&gt;Red Hat Developer Hub is our enterprise-grade, supported version of &lt;a href="https://backstage.io/"&gt;Backstage&lt;/a&gt;, an open source framework (provided by Spotify) for building developer portals. Engineering teams can use Red Hat Developer Hub to reduce friction and frustration and boost their productivity, giving their organization a competitive advantage.&lt;/p&gt; &lt;p&gt;We have preloaded all the necessary Red Hat plug-ins, including a variety of technology and tools that reduce developer cognitive load and support self service, while maintaining context and underlying technologies continuously maintained and improved by platform teams.&lt;/p&gt; &lt;p&gt;Our new initiative, Project Janus, has developed and enhanced the Golden Path templates, Backstage platform, and plug-ins.&lt;/p&gt; &lt;h3&gt;6 Red Hat Plug-ins for Backstage&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/plugins-for-backstage/overview"&gt;Red Hat Plug-ins for Backstage&lt;/a&gt; work in tandem with Red Hat Developer Hub and pre-existing customer installations of Backstage, extending functionality and improving the overall experience. These 6 plug-ins are the supported version of the community plug-ins:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Authentication and Authorization with Keycloak:&lt;/strong&gt; Load users and groups from Keycloak to Backstage, enabling use of multiple authentication providers applied to Backstage entities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multicluster View with Open Cluster Manager (OCM):&lt;/strong&gt; This plug-in provides a multicluster view from Open Cluster Manager’s MultiClusterHub and MultiCluster Engine in Backstage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container Image Registry for Quay:&lt;/strong&gt; The &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image registry for Quay improves the integration and speed of interactions with Quay registries by providing a view into the container image details. This includes Common Vulnerabilities and Exposures (CVEs) associated with deployed images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Application Topology for Kubernetes:&lt;/strong&gt; Consistently visualize relationships and real-time status of applications and workloads deployed to any &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; target, including &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pipelines with Tekton:&lt;/strong&gt; This plug-in provides details of all Tekton Pipelines and their status across all services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitOps with Argo CD:&lt;/strong&gt; Track the health and status of Argo CD and monitor services inside Backstage.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Streamline onboarding with Golden Path templates&lt;/h2&gt; &lt;p&gt;Red Hat Developer Hub provides Golden Path templates that are essential for fast-tracking development and ensuring organization and consistency. These templates are referred to as &lt;strong&gt;golden&lt;/strong&gt; because they provide gold-standard best practices for developers to follow so your code will be coherent, scalable, and maintainable.&lt;/p&gt; &lt;p&gt;You can streamline application and developer onboarding with Golden Path templates. Golden Paths provide pre-architected and supported approaches to building and deploying a particular piece of software without having to learn all the details of the technology used.&lt;/p&gt; &lt;p&gt;Templates enable self-service for developers. They also&lt;strong&gt; &lt;/strong&gt;provide the best pathway to fix bugs or implement features, enabling developers to speed up the process, thereby making your organization more competitive.&lt;/p&gt; &lt;h2&gt;The two sides of Red Hat Developer Hub&lt;/h2&gt; &lt;p&gt;We will explain Red Hat Developer Hub by describing it from two sides—consumption and configuration—and two points of view (i.e., the developer and the &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; administrator.)&lt;/p&gt; &lt;p&gt;From a consumption perspective, Red Hat Developer Hub perfectly matches the functionality needed for the developer as part of the inner loop. For the first time, a developer has a single place to find everything, from links, integrated development environments (IDEs), and GitHub repos, to documentation and components catalogs added to applications.&lt;/p&gt; &lt;h3&gt;How Red Hat Developer Hub benefits developers&lt;/h3&gt; &lt;p&gt;Red Hat Developer Hub provides a single place for a developer to work. Finally, there’s a hub for all things a developer needs, such as resources, clusters, and templates. Red Hat Developer Hub provides complete coverage of the inner loop, IDE, direction interaction, and authentication to GitHub.&lt;/p&gt; &lt;p&gt;Developers must understand the complexity of the various tools they use. Learning numerous toolsets as well as the ins and outs of the frameworks, technologies, and components of their organization can be time consuming. The primary goal of Red Hat Developer Hub is to ease the development process and provide an aggregated toolset that reduces the mental overhead of context switching and searching for the core components they need.&lt;/p&gt; &lt;p&gt;Red Hat Developer Hub provides a wizard and template-driven experience for end developers. This makes it very easy to build complex systems from multiple components, including Git source-driven deployments to systems such as Kubernetes and OpenShift. In addition, the provisioning of these components simplifies the knowledge the developer needs. With a couple of clicks and text entry points, developers can convert a template to a running application component without knowing the convoluted technology beneath it.&lt;/p&gt; &lt;p&gt;This gives developers more time to spend on developing rather than configuring development environments and components. Not only is the developer presented with a rich set of components from which to choose, but system administrators can configure these components.&lt;/p&gt; &lt;h3&gt;How Red Hat Developer Hub benefits administrators&lt;/h3&gt; &lt;p&gt;Red Hat Developer Hub is built on the Janus open source project, which extends the Backstage open source project, providing a highly configurable and secure development portal and catalog to control which components can be built together. Red Hat Developer Hub extends the functionality through a set of predefined supported plug-ins providing extensions and components to ease the developer's path as they design and build composite applications.&lt;/p&gt; &lt;p&gt;Using code-as-config, administrators can control exactly what developers can interact with and at what level, providing a simple to use and simple to configure mechanism for providing developers with a single point for all things development.&lt;/p&gt; &lt;p&gt;Red Hat Developer Hub also provides a highly configurable authentication model, allowing integration with providers such as GitHub and Keycloak. Administrators can configure authentication methods, templates (i.e., multi-component quick starts and API definitions), and the base definitions. This is a sophisticated tool for building organizational developer portals.&lt;/p&gt; &lt;h2&gt;Red Hat Developer Hub increases productivity and more&lt;/h2&gt; &lt;p&gt;Red Hat Developer Hub is designed to significantly improve engineering productivity for IT organizations, enabling development teams to focus on what really matters—writing high-quality code and accelerating application delivery to give organizations a competitive advantage.&lt;/p&gt; &lt;p&gt;Learn more by visiting the &lt;a href="http://developers.redhat.com/products/developer-hub"&gt;Red Hat Developer Hub&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/23/developers-guide-red-hat-developer-hub-and-janus" title="A developer’s guide to Red Hat Developer Hub and Janus"&gt;A developer’s guide to Red Hat Developer Hub and Janus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ian Lawson, Yaina Williams</dc:creator><dc:date>2023-05-23T13:00:00Z</dc:date></entry><entry><title type="html">WildFly 28.0.1 is released!</title><link rel="alternate" href="https://wildfly.org//news/2023/05/23/WildFly2801-Released/" /><author><name>Farah Juma</name></author><id>https://wildfly.org//news/2023/05/23/WildFly2801-Released/</id><updated>2023-05-23T00:00:00Z</updated><content type="html">WildFly 28.0.1.Final is now available . It’s been about five weeks since the WildFly 28 release, so we’ve done a small bug fix update, WildFly 28.0.1. This includes an update to WildFly Preview. The following issues were resolved in 28.0.1: BUGS * [] - todo-backend QS has outdated Readme instructions * [] - Add missing org.jboss.vfs to RESTEasy Spring deployments * [] - todo-backend Readme OpenShift instructions results in a non-functional QS app * [] - LRA causes a failure in the ContextPropagationTestCase * [] - ExpirationMetaData.isExpired() test does not conform to logic in LocalScheduler * [] - Add java.base/java.net package to recommended client side JPMS settings * [] - The JaxrsIntegrationProcessor should not attempt to get the RESTEasy configuration when not a REST deployment. COMPONENT UPGRADES * [] - Upgrade to Smallrye opentelemetry 2.3.2 * [] - Upgrade RESTEasy to 6.2.4.Final * [] - Upgrade xalan to 2.7.3 (CVE-2022-34169) * [] - Upgrade jose4j to 0.9.3 * [] - WildFly Core to 20.0.2.Final Issues resolved in the WildFly Core update included with WildFly 28.0.1 were: BUGS * [] - Changes to json-formatter meta-data never take effect * [] - module java.base does not "opens java.net" to unnamed module TASKS * [] - Add back the org.jboss.vfs module as a dependency on deployments COMPONENT UPGRADES * [] - CVE-2022-1259 Upgrade Undertow to 2.3.6.Final Enjoy!</content><dc:creator>Farah Juma</dc:creator></entry><entry><title type="html">Using Visual Studio to develop and manage WildFly</title><link rel="alternate" href="https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/</id><updated>2023-05-22T09:13:15Z</updated><content type="html">Visual Studio Community Edition is completely free IDE for individual developers. Despite being free, it provides a rich set of features and capabilities comparable to the paid editions of Visual Studio. In this article we will learn how to use it to develop applications on top of WildFly application Server. Harness the Advantages of Visual ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to use OpenShift Data Science for fraud detection</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/22/how-use-openshift-data-science-fraud-detection" /><author><name>Swati Kale</name></author><id>5502a392-3021-4849-9d81-77714a7af992</id><updated>2023-05-22T07:00:00Z</updated><published>2023-05-22T07:00:00Z</published><summary type="html">&lt;p&gt;The problem of detecting fraudulent transactions is intriguing for a data scientist. However, the overhead from setting up the technologies around it can be cumbersome. This is where &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt;, along with &lt;a href="https://www.starburst.io/"&gt;Starburst&lt;/a&gt; and Intel &lt;a href="https://docs.openvino.ai/latest/home.html"&gt;OpenVino&lt;/a&gt;, come to the rescue. Now data scientists can focus on what they do best, model training and crafting; and OpenShift Data Science will do what we do best, providing the tools with the least overhead. &lt;/p&gt; &lt;p&gt;This article will cover detecting fraudulent transactions in a financial institution on Red Hat OpenShift Data Science.&lt;/p&gt; &lt;h2&gt;Workflow for credit card fraud detection&lt;/h2&gt; &lt;p&gt;Credit card fraud is a significant problem in the finance industry. Fraudsters can steal credit card information and use it to make unauthorized purchases. Fraud detection systems can monitor credit card transactions and identify suspicious activities, such as large transactions or transactions in unusual locations, and flag them for further investigation.&lt;/p&gt; &lt;p&gt;Building an effective fraud detection system using machine learning requires careful data collection, feature engineering, model selection, training and validation, deployment, monitoring, and updating to ensure that the system remains effective over time.&lt;/p&gt; &lt;p&gt;The diagram in Figure 1 shows the typical workflow for building and deploying machine learning models for detecting credit card payment fraud.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/workflow_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/workflow_1.png?itok=zQ3ra5xb" width="512" height="57" alt="This diagram shows the typical workflow for building and deploying machine learning models for detecting credit card payment fraud." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: This diagram shows the typical workflow for building and deploying machine learning models for detecting financial fraud.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Overview of the fraud detection solution&lt;/h2&gt; &lt;p&gt;Figure 2 shows how to use the OpenShift Data Science platform to deploy an agile solution for detecting fraudulent credit card payment.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/solution.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/solution.png?itok=4TJY-qMF" width="512" height="310" alt="A diagram of the solution steps using the OpenShift Data Science platform to detect fraudulent credit card payment." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The solution steps using the OpenShift Data Science platform to detect fraudulent credit card payment.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The steps in the diagram are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;The data scientist uploads data to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Enterprise is connected to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;The data scientist uses the query editor in Starburst to preprocess the data and visualize the dataset.&lt;/li&gt; &lt;li aria-level="1"&gt;The cleaned data is uploaded back to Amazon S3 via Starburst.&lt;/li&gt; &lt;li aria-level="1"&gt;Next, the data scientist creates a data science project within OpenShift Data Science which enables them to launch JupyterLab along with specific dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;Retrieves the cleaned data from Amazon S3. Using the data, they train machine learning models and upload them back to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;The trained models are then served by the OpenVINO Model Server.&lt;/li&gt; &lt;li aria-level="1"&gt;Finally, the models are deployed for fraud detection.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;What is Red Hat OpenShift Data Science?&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Data Science is a platform for developing, deploying, and managing machine learning workflows and models in a containerized environment. It is built on top of the Red Hat OpenShift Container Platform and provides a suite of tools and services for ML workflows, including data preparation, model training, and model deployment. Read more about OpenShift Data Science in the article, &lt;a href="https://developers.redhat.com/blog/2021/04/27/4-reasons-youll-love-using-red-hat-openshift-data-science"&gt;4 reasons you’ll love using OpenShift Data Science&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;OpenShift Data Science is fully integrated with AI/ML tools, including &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;JupyterLab&lt;/a&gt; with predefined notebook images for launching notebooks with access to core AI/ML libraries and frameworks like &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt;. It provides a single interface for managing and implementing all ML steps, including model deployment and training, and is backed by local storage for saving tasks for later use.&lt;/p&gt; &lt;p&gt;In our workflow, we created a data connection to Amazon S3 for adding data to the project. OpenShift Data Science is also integrated with Kserve ModelMesh Serving, which provides out-of-the-box integration with model servers and allows selecting the model server and data connection. The user can view the current status of deployed models and their inference endpoints.&lt;/p&gt; &lt;p&gt;We configured the OpenVINO Model Server, which allows for easy deployment and management of pre-trained deep learning models in production environments. It provides a flexible and scalable platform for deploying deep learning models with RESTful APIs, making it easy to integrate the models with applications.&lt;/p&gt; &lt;h2&gt;Why we use Starburst and Amazon S3&lt;/h2&gt; &lt;p&gt;Starburst Enterprise provides a powerful, user-friendly interface for managing Trino clusters, monitoring query performance, and identifying bottlenecks. It is a popular tool for organizations that use Trino for distributed SQL query processing and require a comprehensive interface for managing their Trino clusters. It unlocks access to data where it lives, no data movement required, giving your teams fast and accurate access to more data for analysis using query editor.&lt;/p&gt; &lt;p&gt;We use Amazon S3 for storing the datasets and trained models. The user can make use of the Amazon S3 in data collection, preprocessing data, and model training phase.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;OpenShift Data Science sandbox&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Enterprise license&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Operator installed&lt;/li&gt; &lt;li aria-level="1"&gt;Read and write access to Amazon S3 bucket&lt;/li&gt; &lt;li aria-level="1"&gt;Access to the original dataset&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The environment setup&lt;/h2&gt; &lt;p&gt;You can find guidance for setting up the environment by following these links:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/download"&gt;Set up your Red Hat OpenShift Data Science environment&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;If you haven’t already, you can find information about getting an instance of Red Hat OpenShift Data Science on the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/download"&gt;developer page&lt;/a&gt;. You can spin up your own account on the free OpenShift Data Science Sandbox or learn about installing on your OpenShift cluster.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection/blob/master/Starburst.md"&gt;Set up Starburst operator on OpenShift Data Science&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection/blob/master/Starburst.md"&gt;Connect Starburst to Amazon S3 and launch Starburst Query Editor&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection"&gt;Try out the fraud detection use case on OpenShift Data Science&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Watch the demo video presented by &lt;strong&gt;Suvro Ghosh&lt;/strong&gt; (creator):&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;OpenShift Data Science simplifies fraud detection&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Data Science provides a fully supported environment in which to rapidly develop, train, and test AI/ML models in the public cloud before deploying in production. This use case can be extended by bringing your algorithm for fraud detection and model framework using the OpenShift Data Science ecosystem. Feel free to comment below if you have questions. We welcome your feedback!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/22/how-use-openshift-data-science-fraud-detection" title="How to use OpenShift Data Science for fraud detection"&gt;How to use OpenShift Data Science for fraud detection&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Swati Kale</dc:creator><dc:date>2023-05-22T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.38.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/05/kogito-1-38-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/05/kogito-1-38-0-released.html</id><updated>2023-05-22T05:42:46Z</updated><content type="html">We are glad to announce that the Kogito 1.38.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Data Index addons that allow running indexing capabilities as part of Kogito runtimes, incorporate now the interaction with the Job Service embedded Quarkus extension. * Rocksdb persistence quarkus add-on * Serverless Workflow embedded executor dependency list has been substantially reduced. * Added support for full GVK to Knative custom function * Added support for existing “quarkus.flyway.locations” values  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.28.0 artifacts are available at the . A detailed changelog for 1.38.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>How to use RHEL application compatibility guidelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" /><author><name>Carlos O'Donell</name></author><id>6da035d9-239a-4926-aeaa-77409c970e44</id><updated>2023-05-18T07:00:00Z</updated><published>2023-05-18T07:00:00Z</published><summary type="html">&lt;p&gt;In this three-part series, we explore the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; published application compatibility guidelines (ACG), and how developers can use them to ensure their application remains compatible with future releases of RHEL. Building applications can be difficult, and building applications that continue to operate after an in-place distribution upgrade is even harder. How does Red Hat Enterprise Linux make it easier? It provides guidelines and guarantees that you can follow to improve application compatibility.&lt;/p&gt; &lt;p&gt;In this article, we will expand on the concept of application compatibility. In the second part, we will review the topic of compatibility with more examples. In the third article, we will discuss container userspace compatibility with the host kernel services.&lt;/p&gt; &lt;h2&gt;What is application compatibility?&lt;/h2&gt; &lt;p&gt;What we call application compatibility is traditionally referred to as backwards compatibility. It is the ability to run an unmodified application binary on the current or newer version of the distribution and have it operate correctly (i.e., compatible with the release of the distribution).&lt;/p&gt; &lt;h3&gt;Maintaining compatibility&lt;/h3&gt; &lt;p&gt;Application compatibility is maintained by ensuring that the dependencies of the application continue to be provided and that the application continues to function as intended.&lt;/p&gt; &lt;p&gt;When we talk about &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt; applications, this could mean that the libraries the application needs are still present and providing the expected set of features and behaviors. When we talk about &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, it means continuing to provide the modules the Python script requires or the language features it needs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;Red Hat Enterprise Linux 9&lt;/a&gt; was released in May of 2022. At the same time, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG) was published, and the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM) was updated. These two guides are key to helping developers learn about the guidelines they can follow to ensure application compatibility with a given RHEL release.&lt;/p&gt; &lt;p&gt;In the first two parts of this series, we will focus on the RHEL ACG.&lt;/p&gt; &lt;h2&gt;Defining API and ABI&lt;/h2&gt; &lt;p&gt;This is a quick recap of the detailed definitions in the compatibility guide. An API is the application programming interface, and it represents the set of conventions, features, or behaviors at compile time. An ABI is the application binary interface, and it represents the set of conventions, features, or behaviors at run time.&lt;/p&gt; &lt;p&gt;An API can be a source level function call (e.g.,&lt;code&gt;exit(0)&lt;/code&gt;). An ABI can be the actual implementation of the &lt;code&gt;void exit(int status)&lt;/code&gt; function in the C library.&lt;/p&gt; &lt;h2&gt;Components of the application compatibility guide&lt;/h2&gt; &lt;p&gt;The ACG is split into two major sections:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Guidelines for preserving application compatibility across minor and major RHEL versions.&lt;/li&gt; &lt;li aria-level="1"&gt;The binary rpm package list and the compatibility guarantees.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The distribution places the binary rpm packages into one of four compatibility levels. It can be viewed as a narrowing set of compatibility guarantees across minor and major upgrade paths.&lt;/p&gt; &lt;div&gt; &lt;table border="1" cellspacing="0" width="672"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;⇓ Compatibility Level / RHEL Version ⇒&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+2&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;2&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;3&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI defined by life cycle&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;4&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI subject to change at any time&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt;Following the guidelines and using only packages that provide the guarantees your application needs will ensure that your application remains as compatible as possible across minor and major RHEL version upgrades.&lt;/p&gt; &lt;p&gt;To be compatible with minor version upgrades, it requires you to use only packages in compatibility level 2 or level 1, with review of packages used in level 3. To be compatible with major version upgrades, it requires you to use only packages in compatibility level 1, with review of packages used in level 3.&lt;/p&gt; &lt;p&gt;The default for packages in RHEL is compatibility level 2, which ensures that applications keep working across minor version upgrades.&lt;/p&gt; &lt;h2&gt;Workloads, services, containers, and packages&lt;/h2&gt; &lt;p&gt;You’ll notice that we talk a lot about packages. We talk about packages because it allows developers to decide which parts of decomposed workloads will be compatible with RHEL for a long time and which parts you might have to recompile, rewrite, or forward port.&lt;/p&gt; &lt;p&gt;At the highest level, you are going to have a workload that you want to support over time. That workload does something useful. Workloads can be managed with &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; like &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; or &lt;a href="https://backstage.io/"&gt;Backstage&lt;/a&gt;. I am not going to talk about workloads because as an abstraction, they are useful for talking at a very high level. When you think about it, the workload of “transactional request processing system” is too abstract for us to talk about compatibility.&lt;/p&gt; &lt;p&gt;A workload can be decomposed into services, and at this point it starts getting closer to the level at which we are talking about cross-RHEL-release compatibility guidelines. Services have concrete instantiations like an AMQP (Advanced Message Queuing Protocol) server running locally that handles messages (e.g., &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;When it comes to the smallest installable unit of something on RHEL, we can talk generally about containers or rpm packages. I’m going to defer the conversation about containers until part 3 of this series since the compatibility of containers is covered in the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So either we are talking about the rpm packages in the container, or we’re talking about rpm packages on the host. As a developer you are still responsible for the decomposition of the workload and services into packages (software collections or modules are still delivered as packages). If packages change between major version upgrades then &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/upgrading_from_rhel_8_to_rhel_9/planning-an-upgrade_upgrading-from-rhel-8-to-rhel-9"&gt;RHEL Leapp&lt;/a&gt; is there to help ensure the same features are present even if the package changes names.&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C&lt;/h2&gt; &lt;p&gt;Explaining application compatibility guidelines is easier with an example. Say you are building an application in C that you started developing on RHEL 7, and you are now looking at RHEL 8 and RHEL 9 for eventual deployment.&lt;/p&gt; &lt;p&gt;Let’s dive in by using a simple C “Hello World” example and see what the application compatibility guidelines say about each of the development steps in building the application.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compile, inspect, and run as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o helloworld helloworld.c $ ldd helloworld linux-vdso.so.1 =&gt; (0x00007ffd09d75000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fe51a633000) /lib64/ld-linux-x86-64.so.2 (0x00007fe51aa01000) $ readelf -W --dyn-syms helloworld Symbol table '.dynsym' contains 4 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 (2) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 3: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are several important takeaways from this example such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Links only the libraries it needs (ClLibrary: glibc, implicitly). &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility with future RHEL versions by limiting library use.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Uses C library header files for the APIs it uses (i.e., &lt;strong&gt;#include&lt;/strong&gt; directive). &lt;ul&gt;&lt;li aria-level="2"&gt;Ensures the development packages are installed provide those files and ensures any compatibility mechanisms are active.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Application is executed in an environment that is as new as the system it was compiled on. &lt;ul&gt;&lt;li aria-level="2"&gt;Backwards compatibility is guaranteed.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Does not use static linking. &lt;ul&gt;&lt;li aria-level="2"&gt;Robust runtime behavior of statically linked applications requires that the runtime environment match exactly the build time environment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Avoids explicit dependency on a given Linux kernel version. &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility if the application is later packaged as a container.&lt;/li&gt; &lt;li aria-level="2"&gt;Improves compatibility with future RHEL kernel versions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;a href="https://access.redhat.com/articles/rhel-abi-compatibility"&gt;RHEL7 Application Compatibility Guide&lt;/a&gt; recommends all these points and more.&lt;/p&gt; &lt;p&gt;So you might be asking, “Great, but what does that mean for my RHEL 8 and RHEL 9 migration?” Let's look first at the dependencies of the application. In this case, it’s only two dynamic symbols in the rpm package glibc, i.e. &lt;em&gt;&lt;strong&gt;puts@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;__libc_start_main@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt;. Tracking which package provides these symbols is done by rpm and dynamic shared object names, and symbol set provides. It is a topic for another article, so for now, we'll skip this part.&lt;/p&gt; &lt;p&gt;The glibc binary rpm package is one of a small set of packages that is in compatibility level 1, and these are very important packages that are guaranteed to have a compatible ABI for at least three major RHEL releases. That means that the application we just created in this example should run correctly in RHEL 7, RHEL 8, and RHEL 9. Let's try it out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On RHEL 7:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 3.10.0-1160.88.1.el7.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 8:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 4.18.0-468.el8.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 9:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 5.14.0-162.21.1.el9_1.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having an application that only depends on compatibility level 1 packages allows the binary to run across three major RHEL releases without any issues. By 2024, that will be ten years of runtime compatibility!&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C using OpenSSL&lt;/h2&gt; &lt;p&gt;Let us take that example a bit further and try to use a library like OpenSSL that is only compatibility level 2, which means the ABI guarantee is only for the minor version upgrades of RHEL. That means that if you compile on RHEL 7.0, the application should still be working in RHEL 7.9 (last y-stream release), but is not guaranteed to work in RHEL 8.0 without recompilation in that distribution.&lt;/p&gt; &lt;p&gt;The example program here uses OpenSSL’s BIO interface to read from standard input and write the same thing to standard output. While this doesn’t exercise all of OpenSSL’s features, it does help us illustrate application compatibility.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;openssl/ssl.h&gt; #include &lt;openssl/bio.h&gt; #include &lt;stdlib.h&gt; #define BUFSIZE 4096 int main(void) { char buf[BUFSIZE]; int bin, bout; BIO *bio_stdin, *bio_stdout; bio_stdin = BIO_new_fp(stdin, BIO_NOCLOSE); bio_stdout = BIO_new_fp(stdout, BIO_NOCLOSE); if (bio_stdin == NULL || bio_stdout == NULL) exit (1); while ((bin = BIO_read (bio_stdin, buf, BUFSIZE)) &gt; 0) { bout = BIO_write (bio_stdout, buf, bin); if (bin != bout) exit (1); } BIO_free (bio_stdout); BIO_free (bio_stdin); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compiled, inspected, and run like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o bio-cp bio-cp.c -lssl -lcrypto $ ldd./bio-cp linux-vdso.so.1 =&gt; (0x00007ffc4cfa1000) libssl.so.10 =&gt; /lib64/libssl.so.10 (0x00007f1811c65000) libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007f1811802000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f1811434000) libgssapi_krb5.so.2 =&gt; /lib64/libgssapi_krb5.so.2 (0x00007f18111e7000) libkrb5.so.3 =&gt; /lib64/libkrb5.so.3 (0x00007f1810efe000) libcom_err.so.2 =&gt; /lib64/libcom_err.so.2 (0x00007f1810cfa000) libk5crypto.so.3 =&gt; /lib64/libk5crypto.so.3 (0x00007f1810ac7000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f18108c3000) libz.so.1 =&gt; /lib64/libz.so.1 (0x00007f18106ad000) /lib64/ld-linux-x86-64.so.2 (0x00007f1811ed7000) libkrb5support.so.0 =&gt; /lib64/libkrb5support.so.0 (0x00007f181049d000) libkeyutils.so.1 =&gt; /lib64/libkeyutils.so.1 (0x00007f1810299000) libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007f181007f000) libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f180fe63000) libselinux.so.1 =&gt; /lib64/libselinux.so.1 (0x00007f180fc3c000) libpcre.so.1 =&gt; /lib64/libpcre.so.1 (0x00007f180f9da000) $ readelf -W --dyn-syms bio-cp Symbol table '.dynsym' contains 15 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_write@libcrypto.so.10 (3) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_read@libcrypto.so.10 (3) 3: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_free@libcrypto.so.10 (3) 4: 0000000000000000 0 FUNC GLOBAL DEFAULT UND exit@GLIBC_2.2.5 (2) 5: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_new_fp@libcrypto.so.10 (3) 6: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 7: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 8: 0000000000601060 8 OBJECT GLOBAL DEFAULT 25 stdout@GLIBC_2.2.5 (2) 9: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 24 _edata 10: 0000000000601078 0 NOTYPE GLOBAL DEFAULT 25 _end 11: 0000000000601068 8 OBJECT GLOBAL DEFAULT 25 stdin@GLIBC_2.2.5 (2) 12: 0000000000400640 0 FUNC GLOBAL DEFAULT 11 _init 13: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 25 __bss_start 14: 0000000000400914 0 FUNC GLOBAL DEFAULT 14 _fini $./bio-cp &lt; helloworld.c #include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; } $ echo $? 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application compatibility guidelines for RHEL 7 state that to be compatible, you must recompile at each major release to ensure ABI compatibility. Again, this is because OpenSSL is in compatibility level 2. Let's put this to the test.&lt;/p&gt; &lt;p&gt;On RHEL 8:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 9:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The default version of OpenSSL in RHEL7 is 1.0. In RHEL8 it is 1.1.1, and in RHEL9 it is 3.0. Each version is unique, and to use them requires the application to be compiled against the specific version in the distribution.&lt;/p&gt; &lt;p&gt;The application compatibility guide lists OpenSSL as compatibility level 2, and it bears repeating that such libraries have guaranteed ABI compatibility only within the major version of RHEL in which they were released.&lt;/p&gt; &lt;p&gt;The recent &lt;a href="https://www.redhat.com/en/blog/experience-bringing-openssl-30-rhel-and-fedora"&gt;migration of RHEL9 to OpenSSL 3.0&lt;/a&gt; was a technically complex migration. Red Hat went above and beyond by providing customers with compatibility packages to facilitate application developers. These compatibility packages provide the libraries required to meet the ABI requirements of OpenSSL using applications. Similar packages were also provided in RHEL 8 to enable the migration from OpenSSL 1.0 to 1.1.1. Lets try out the compatibility packages.&lt;/p&gt; &lt;p&gt;On RHEL 8 with compat-openssl10:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf install compat-openssl10 ... $ ldd bio-cp linux-vdso.so.1 (0x00007fff5df5b000) libssl.so.10 =&gt; /lib64/libssl.so.10 (0x00007fac6d7ed000) libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007fac6d38b000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fac6cfc6000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fac6cdc2000) libz.so.1 =&gt; /lib64/libz.so.1 (0x00007fac6cbaa000) /lib64/ld-linux-x86-64.so.2 (0x00007fac6da5c000) $./bio-cp &lt; helloworld.c #include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While the ACG says not to rely on such compatibility for compatibility level 2 packages, there are cases like this one with OpenSSL where Red Hat has gone above and beyond to ensure customer success.&lt;/p&gt; &lt;h2&gt;Learn more about RHEL ACG&lt;/h2&gt; &lt;p&gt;We have discussed application compatibility, and how Red Hat provides guidelines and guarantees to help developers create applications that are compatible with minor and major version upgrades for Red Hat Enterprise Linux. We have looked at the first important document that provides those guidelines for RHEL 9, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG). We have looked at two examples that showcase application compatibility across major version upgrades.&lt;/p&gt; &lt;p&gt;I encourage all developers out there to read the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG), and make sure you are following the best practice guidelines to meet the compatibility needs of your application, service, or workload. Stay tuned for part 2 of this series where we will look again at the RHEL ACG, but dive into more complex examples. Part 3 will cover container compatibility and describe the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM).&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" title="How to use RHEL application compatibility guidelines"&gt;How to use RHEL application compatibility guidelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Carlos O'Donell</dc:creator><dc:date>2023-05-18T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - May, 15 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-05-18.html" /><category term="quarkus" /><category term="java" /><category term="wildfly" /><category term="camel" /><category term="strimzi" /><category term="podman" /><author><name>Francesco Marchioni</name><uri>https://www.jboss.org/people/francesco-marchioni</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-05-18.html</id><updated>2023-05-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, wildfly, camel, strimzi, podman"&gt; &lt;h1&gt;This Week in JBoss - May, 15 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Happy Friday, everyone!&lt;/p&gt; &lt;p&gt;Here is another edition of the JBoss Editorial with exciting news and updates from your JBoss communities.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the most recent releases for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-3-final-released/"&gt;Quarkus 3.0.3.Final released&lt;/a&gt; - The Quarkus Team released Quarkus 3.0.3.Final, as part of the second maintenance release of our 3.0 release train. This release contains bugfixes and documentation improvements.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/kogito-1-37-0-released.html"&gt;Kogito 1.37 released&lt;/a&gt; - The new Kogito release features enhancements in the flow actions, workflow definitions logging and service discovery.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/releases/release-4.0.0-M3/"&gt;Camel RELEASE 4.0.0-M3 available&lt;/a&gt; - The Camel community announces the availability of Camel 4.0.0-M3, the third milestone towards a new 4.0.0 major release which comes with 155 new features and improvements.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_exactly_once_semantics_with_kafka_transactions"&gt;Exactly-once semantics with Kafka transactions&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://strimzi.io/blog/2023/05/03/kafka-transactions/"&gt;Exactly-once semantics with Kafka transactions&lt;/a&gt;, by Federico Valeri.&lt;/p&gt; &lt;p&gt;Kafka transactions play a vital role in guaranteeing the reliability and integrity of data, making them a pivotal component of the Kafka platform. Nevertheless, these benefits are accompanied by a trade-off in terms of decreased throughput and added latency, necessitating potential adjustments. Neglecting to monitor transactions that remain unresolved can adversely affect the availability of the service. This article sheds some light on this topic.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_use_the_new_openshift_quick_starts_to_deploy_jboss_eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/a&gt;, by Philip Hayes&lt;/p&gt; &lt;p&gt;This article showcases the latest JBoss EAP quick start, specifically created to assist developers who are already familiar with conventional JBoss EAP deployments. Its purpose is to provide a comprehensive walkthrough on constructing and deploying application images on OpenShift. The quick start offers valuable guidance on utilizing Helm to generate the necessary build configs, deployment configs, and external routes for building and deploying JBoss EAP applications on OpenShift.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_integrate_excel_with_drools_on_openshift_with_knative_and_quarkus"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/a&gt;, by Matteo Mortari&lt;/p&gt; &lt;p&gt;In this blog post, Matteo shares the results of a technical exploration of bringing together different technologies and platforms. At the end of the day, he combined things like regular spreadsheet applications (like Excel), serverless platforms (Knative on OpenShift), and our rule engine Drools to see how they could work together.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_podman_desktop_beginners_guide"&gt;Podman Desktop Beginner’s Guide&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.mastertheboss.com/soa-cloud/docker/podman-desktop-a-beginners-guide-to-containerization/"&gt;Podman Desktop Guide&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;In this tutorial, I’m introducing the Podman desktop UI which simplifies the usage and management of container images. As an example, we will learn how to pull, start, and monitor a WildFly Container image with just a few clicks!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_whats_new_in_red_hats_migration_toolkit_for_applications_6_1"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/10/whats-new-red-hats-migration-toolkit-applications-61"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/a&gt;, by Yashwanth Maheshwaram&lt;/p&gt; &lt;p&gt;Red Hat Migration Toolkit is an essential tool to simplify the upgrade or migration of a large set of enterprise applications. This article gives you an update with the latest news and includes a great demo video.&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/francesco-marchioni.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Francesco Marchioni&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Francesco Marchioni</dc:creator></entry></feed>
